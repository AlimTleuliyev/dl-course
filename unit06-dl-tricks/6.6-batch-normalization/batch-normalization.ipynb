{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization is a technique to improve the training speed of a neural network. It does so by normalizing the output of the previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This is done for each feature independently. The normalized output is then scaled by a learned parameter gamma and shifted by a learned parameter beta. These parameters are learned during training. Batch Normalization is usually applied after the activation function.\n",
    "\n",
    "It is used to prevent overfitting and to speed up the training process. It also allows for higher learning rates. Batch Normalization is used in almost all state-of-the-art neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](pic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.BatchNorm1d(20),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(20, 30),\n",
    "    nn.BatchNorm1d(30),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(30, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>class label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781306</td>\n",
       "      <td>1.062984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.060524</td>\n",
       "      <td>-1.095550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.632125</td>\n",
       "      <td>0.674028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.424712</td>\n",
       "      <td>0.535203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.383161</td>\n",
       "      <td>1.368510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0.792484</td>\n",
       "      <td>0.839275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0.582466</td>\n",
       "      <td>-0.749250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>-1.593475</td>\n",
       "      <td>0.671721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>-0.812671</td>\n",
       "      <td>-0.268542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>-1.286524</td>\n",
       "      <td>0.655459</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2  class label\n",
       "0    0.781306  1.062984            0\n",
       "1   -1.060524 -1.095550            0\n",
       "2    0.632125  0.674028            0\n",
       "3   -1.424712  0.535203            1\n",
       "4    1.383161  1.368510            0\n",
       "..        ...       ...          ...\n",
       "745  0.792484  0.839275            0\n",
       "746  0.582466 -0.749250            1\n",
       "747 -1.593475  0.671721            1\n",
       "748 -0.812671 -0.268542            0\n",
       "749 -1.286524  0.655459            1\n",
       "\n",
       "[750 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/xor.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x1', 'x2']].values\n",
    "y = df['class label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 42, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (573, 2)\n",
      "X_val.shape = (64, 2)\n",
      "X_test.shape = (113, 2)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape =', X_train.shape)\n",
    "print('X_val.shape =', X_val.shape)\n",
    "print('X_test.shape =', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels: [287 286]\n",
      "Validation labels: [32 32]\n",
      "Test labels: [57 56]\n"
     ]
    }
   ],
   "source": [
    "print('Training labels:', np.bincount(y_train))\n",
    "print('Validation labels:', np.bincount(y_val))\n",
    "print('Test labels:', np.bincount(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(\n",
    "    X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
    "    marker = 'D', markersize = 6, label = 'class 0', linestyle = ''\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
    "    marker = '^', markersize = 6, label = 'class 1', linestyle = ''\n",
    ")\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.xlabel('Feature $x_1$')\n",
    "plt.ylabel('Feature $x_2$')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see to classes are not linearly separable so Logistic Regression will not work here. We need to use a non-linear model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_feautures, num_classes):\n",
    "        super().__init__()\n",
    "        self.all_layers = nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            nn.Linear(num_feautures, 25),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Output layer\n",
    "            nn.Linear(15, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.all_layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.feautures = torch.tensor(X, dtype = torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype = torch.int64)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.feautures[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_ds = MyDataset(X_train, y_train)\n",
    "val_ds = MyDataset(X_val, y_val)\n",
    "test_ds = MyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size = 32, shuffle = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size = 32, shuffle = False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size = 32, shuffle = False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct_pred, num_examples = 0, 0\n",
    "\n",
    "    for features, labels in data_loader:\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim = 1)\n",
    "        correct_pred += torch.sum(predictions == labels)\n",
    "        num_examples += len(labels)\n",
    "    \n",
    "    return correct_pred / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 000/018 | Train/Val Loss: 0.71\n",
      "Epoch: 001/100 | Batch 001/018 | Train/Val Loss: 0.70\n",
      "Epoch: 001/100 | Batch 002/018 | Train/Val Loss: 0.71\n",
      "Epoch: 001/100 | Batch 003/018 | Train/Val Loss: 0.72\n",
      "Epoch: 001/100 | Batch 004/018 | Train/Val Loss: 0.70\n",
      "Epoch: 001/100 | Batch 005/018 | Train/Val Loss: 0.68\n",
      "Epoch: 001/100 | Batch 006/018 | Train/Val Loss: 0.68\n",
      "Epoch: 001/100 | Batch 007/018 | Train/Val Loss: 0.69\n",
      "Epoch: 001/100 | Batch 008/018 | Train/Val Loss: 0.69\n",
      "Epoch: 001/100 | Batch 009/018 | Train/Val Loss: 0.68\n",
      "Epoch: 001/100 | Batch 010/018 | Train/Val Loss: 0.67\n",
      "Epoch: 001/100 | Batch 011/018 | Train/Val Loss: 0.67\n",
      "Epoch: 001/100 | Batch 012/018 | Train/Val Loss: 0.67\n",
      "Epoch: 001/100 | Batch 013/018 | Train/Val Loss: 0.68\n",
      "Epoch: 001/100 | Batch 014/018 | Train/Val Loss: 0.67\n",
      "Epoch: 001/100 | Batch 015/018 | Train/Val Loss: 0.66\n",
      "Epoch: 001/100 | Batch 016/018 | Train/Val Loss: 0.66\n",
      "Epoch: 001/100 | Batch 017/018 | Train/Val Loss: 0.67\n",
      "Epoch: 001/100 Train Acc: 0.951 Val Acc: 0.906\n",
      "Epoch: 002/100 | Batch 000/018 | Train/Val Loss: 0.66\n",
      "Epoch: 002/100 | Batch 001/018 | Train/Val Loss: 0.66\n",
      "Epoch: 002/100 | Batch 002/018 | Train/Val Loss: 0.65\n",
      "Epoch: 002/100 | Batch 003/018 | Train/Val Loss: 0.66\n",
      "Epoch: 002/100 | Batch 004/018 | Train/Val Loss: 0.65\n",
      "Epoch: 002/100 | Batch 005/018 | Train/Val Loss: 0.64\n",
      "Epoch: 002/100 | Batch 006/018 | Train/Val Loss: 0.65\n",
      "Epoch: 002/100 | Batch 007/018 | Train/Val Loss: 0.65\n",
      "Epoch: 002/100 | Batch 008/018 | Train/Val Loss: 0.65\n",
      "Epoch: 002/100 | Batch 009/018 | Train/Val Loss: 0.64\n",
      "Epoch: 002/100 | Batch 010/018 | Train/Val Loss: 0.64\n",
      "Epoch: 002/100 | Batch 011/018 | Train/Val Loss: 0.63\n",
      "Epoch: 002/100 | Batch 012/018 | Train/Val Loss: 0.63\n",
      "Epoch: 002/100 | Batch 013/018 | Train/Val Loss: 0.63\n",
      "Epoch: 002/100 | Batch 014/018 | Train/Val Loss: 0.64\n",
      "Epoch: 002/100 | Batch 015/018 | Train/Val Loss: 0.63\n",
      "Epoch: 002/100 | Batch 016/018 | Train/Val Loss: 0.63\n",
      "Epoch: 002/100 | Batch 017/018 | Train/Val Loss: 0.61\n",
      "Epoch: 002/100 Train Acc: 0.963 Val Acc: 0.938\n",
      "Epoch: 003/100 | Batch 000/018 | Train/Val Loss: 0.62\n",
      "Epoch: 003/100 | Batch 001/018 | Train/Val Loss: 0.62\n",
      "Epoch: 003/100 | Batch 002/018 | Train/Val Loss: 0.61\n",
      "Epoch: 003/100 | Batch 003/018 | Train/Val Loss: 0.61\n",
      "Epoch: 003/100 | Batch 004/018 | Train/Val Loss: 0.60\n",
      "Epoch: 003/100 | Batch 005/018 | Train/Val Loss: 0.61\n",
      "Epoch: 003/100 | Batch 006/018 | Train/Val Loss: 0.60\n",
      "Epoch: 003/100 | Batch 007/018 | Train/Val Loss: 0.60\n",
      "Epoch: 003/100 | Batch 008/018 | Train/Val Loss: 0.61\n",
      "Epoch: 003/100 | Batch 009/018 | Train/Val Loss: 0.59\n",
      "Epoch: 003/100 | Batch 010/018 | Train/Val Loss: 0.58\n",
      "Epoch: 003/100 | Batch 011/018 | Train/Val Loss: 0.59\n",
      "Epoch: 003/100 | Batch 012/018 | Train/Val Loss: 0.58\n",
      "Epoch: 003/100 | Batch 013/018 | Train/Val Loss: 0.60\n",
      "Epoch: 003/100 | Batch 014/018 | Train/Val Loss: 0.58\n",
      "Epoch: 003/100 | Batch 015/018 | Train/Val Loss: 0.57\n",
      "Epoch: 003/100 | Batch 016/018 | Train/Val Loss: 0.59\n",
      "Epoch: 003/100 | Batch 017/018 | Train/Val Loss: 0.58\n",
      "Epoch: 003/100 Train Acc: 0.965 Val Acc: 0.953\n",
      "Epoch: 004/100 | Batch 000/018 | Train/Val Loss: 0.57\n",
      "Epoch: 004/100 | Batch 001/018 | Train/Val Loss: 0.55\n",
      "Epoch: 004/100 | Batch 002/018 | Train/Val Loss: 0.57\n",
      "Epoch: 004/100 | Batch 003/018 | Train/Val Loss: 0.57\n",
      "Epoch: 004/100 | Batch 004/018 | Train/Val Loss: 0.57\n",
      "Epoch: 004/100 | Batch 005/018 | Train/Val Loss: 0.55\n",
      "Epoch: 004/100 | Batch 006/018 | Train/Val Loss: 0.54\n",
      "Epoch: 004/100 | Batch 007/018 | Train/Val Loss: 0.55\n",
      "Epoch: 004/100 | Batch 008/018 | Train/Val Loss: 0.53\n",
      "Epoch: 004/100 | Batch 009/018 | Train/Val Loss: 0.55\n",
      "Epoch: 004/100 | Batch 010/018 | Train/Val Loss: 0.54\n",
      "Epoch: 004/100 | Batch 011/018 | Train/Val Loss: 0.56\n",
      "Epoch: 004/100 | Batch 012/018 | Train/Val Loss: 0.52\n",
      "Epoch: 004/100 | Batch 013/018 | Train/Val Loss: 0.53\n",
      "Epoch: 004/100 | Batch 014/018 | Train/Val Loss: 0.53\n",
      "Epoch: 004/100 | Batch 015/018 | Train/Val Loss: 0.55\n",
      "Epoch: 004/100 | Batch 016/018 | Train/Val Loss: 0.53\n",
      "Epoch: 004/100 | Batch 017/018 | Train/Val Loss: 0.53\n",
      "Epoch: 004/100 Train Acc: 0.972 Val Acc: 0.953\n",
      "Epoch: 005/100 | Batch 000/018 | Train/Val Loss: 0.51\n",
      "Epoch: 005/100 | Batch 001/018 | Train/Val Loss: 0.53\n",
      "Epoch: 005/100 | Batch 002/018 | Train/Val Loss: 0.50\n",
      "Epoch: 005/100 | Batch 003/018 | Train/Val Loss: 0.50\n",
      "Epoch: 005/100 | Batch 004/018 | Train/Val Loss: 0.50\n",
      "Epoch: 005/100 | Batch 005/018 | Train/Val Loss: 0.50\n",
      "Epoch: 005/100 | Batch 006/018 | Train/Val Loss: 0.50\n",
      "Epoch: 005/100 | Batch 007/018 | Train/Val Loss: 0.48\n",
      "Epoch: 005/100 | Batch 008/018 | Train/Val Loss: 0.46\n",
      "Epoch: 005/100 | Batch 009/018 | Train/Val Loss: 0.49\n",
      "Epoch: 005/100 | Batch 010/018 | Train/Val Loss: 0.47\n",
      "Epoch: 005/100 | Batch 011/018 | Train/Val Loss: 0.45\n",
      "Epoch: 005/100 | Batch 012/018 | Train/Val Loss: 0.50\n",
      "Epoch: 005/100 | Batch 013/018 | Train/Val Loss: 0.46\n",
      "Epoch: 005/100 | Batch 014/018 | Train/Val Loss: 0.45\n",
      "Epoch: 005/100 | Batch 015/018 | Train/Val Loss: 0.50\n",
      "Epoch: 005/100 | Batch 016/018 | Train/Val Loss: 0.51\n",
      "Epoch: 005/100 | Batch 017/018 | Train/Val Loss: 0.44\n",
      "Epoch: 005/100 Train Acc: 0.976 Val Acc: 0.938\n",
      "Epoch: 006/100 | Batch 000/018 | Train/Val Loss: 0.49\n",
      "Epoch: 006/100 | Batch 001/018 | Train/Val Loss: 0.43\n",
      "Epoch: 006/100 | Batch 002/018 | Train/Val Loss: 0.40\n",
      "Epoch: 006/100 | Batch 003/018 | Train/Val Loss: 0.40\n",
      "Epoch: 006/100 | Batch 004/018 | Train/Val Loss: 0.46\n",
      "Epoch: 006/100 | Batch 005/018 | Train/Val Loss: 0.42\n",
      "Epoch: 006/100 | Batch 006/018 | Train/Val Loss: 0.43\n",
      "Epoch: 006/100 | Batch 007/018 | Train/Val Loss: 0.41\n",
      "Epoch: 006/100 | Batch 008/018 | Train/Val Loss: 0.43\n",
      "Epoch: 006/100 | Batch 009/018 | Train/Val Loss: 0.42\n",
      "Epoch: 006/100 | Batch 010/018 | Train/Val Loss: 0.46\n",
      "Epoch: 006/100 | Batch 011/018 | Train/Val Loss: 0.41\n",
      "Epoch: 006/100 | Batch 012/018 | Train/Val Loss: 0.39\n",
      "Epoch: 006/100 | Batch 013/018 | Train/Val Loss: 0.40\n",
      "Epoch: 006/100 | Batch 014/018 | Train/Val Loss: 0.39\n",
      "Epoch: 006/100 | Batch 015/018 | Train/Val Loss: 0.39\n",
      "Epoch: 006/100 | Batch 016/018 | Train/Val Loss: 0.37\n",
      "Epoch: 006/100 | Batch 017/018 | Train/Val Loss: 0.41\n",
      "Epoch: 006/100 Train Acc: 0.977 Val Acc: 0.953\n",
      "Epoch: 007/100 | Batch 000/018 | Train/Val Loss: 0.41\n",
      "Epoch: 007/100 | Batch 001/018 | Train/Val Loss: 0.33\n",
      "Epoch: 007/100 | Batch 002/018 | Train/Val Loss: 0.34\n",
      "Epoch: 007/100 | Batch 003/018 | Train/Val Loss: 0.37\n",
      "Epoch: 007/100 | Batch 004/018 | Train/Val Loss: 0.37\n",
      "Epoch: 007/100 | Batch 005/018 | Train/Val Loss: 0.35\n",
      "Epoch: 007/100 | Batch 006/018 | Train/Val Loss: 0.42\n",
      "Epoch: 007/100 | Batch 007/018 | Train/Val Loss: 0.35\n",
      "Epoch: 007/100 | Batch 008/018 | Train/Val Loss: 0.34\n",
      "Epoch: 007/100 | Batch 009/018 | Train/Val Loss: 0.33\n",
      "Epoch: 007/100 | Batch 010/018 | Train/Val Loss: 0.36\n",
      "Epoch: 007/100 | Batch 011/018 | Train/Val Loss: 0.30\n",
      "Epoch: 007/100 | Batch 012/018 | Train/Val Loss: 0.33\n",
      "Epoch: 007/100 | Batch 013/018 | Train/Val Loss: 0.32\n",
      "Epoch: 007/100 | Batch 014/018 | Train/Val Loss: 0.35\n",
      "Epoch: 007/100 | Batch 015/018 | Train/Val Loss: 0.34\n",
      "Epoch: 007/100 | Batch 016/018 | Train/Val Loss: 0.32\n",
      "Epoch: 007/100 | Batch 017/018 | Train/Val Loss: 0.26\n",
      "Epoch: 007/100 Train Acc: 0.983 Val Acc: 0.984\n",
      "Epoch: 008/100 | Batch 000/018 | Train/Val Loss: 0.28\n",
      "Epoch: 008/100 | Batch 001/018 | Train/Val Loss: 0.30\n",
      "Epoch: 008/100 | Batch 002/018 | Train/Val Loss: 0.29\n",
      "Epoch: 008/100 | Batch 003/018 | Train/Val Loss: 0.31\n",
      "Epoch: 008/100 | Batch 004/018 | Train/Val Loss: 0.30\n",
      "Epoch: 008/100 | Batch 005/018 | Train/Val Loss: 0.30\n",
      "Epoch: 008/100 | Batch 006/018 | Train/Val Loss: 0.29\n",
      "Epoch: 008/100 | Batch 007/018 | Train/Val Loss: 0.25\n",
      "Epoch: 008/100 | Batch 008/018 | Train/Val Loss: 0.29\n",
      "Epoch: 008/100 | Batch 009/018 | Train/Val Loss: 0.30\n",
      "Epoch: 008/100 | Batch 010/018 | Train/Val Loss: 0.25\n",
      "Epoch: 008/100 | Batch 011/018 | Train/Val Loss: 0.27\n",
      "Epoch: 008/100 | Batch 012/018 | Train/Val Loss: 0.28\n",
      "Epoch: 008/100 | Batch 013/018 | Train/Val Loss: 0.29\n",
      "Epoch: 008/100 | Batch 014/018 | Train/Val Loss: 0.27\n",
      "Epoch: 008/100 | Batch 015/018 | Train/Val Loss: 0.27\n",
      "Epoch: 008/100 | Batch 016/018 | Train/Val Loss: 0.20\n",
      "Epoch: 008/100 | Batch 017/018 | Train/Val Loss: 0.23\n",
      "Epoch: 008/100 Train Acc: 0.984 Val Acc: 0.984\n",
      "Epoch: 009/100 | Batch 000/018 | Train/Val Loss: 0.23\n",
      "Epoch: 009/100 | Batch 001/018 | Train/Val Loss: 0.25\n",
      "Epoch: 009/100 | Batch 002/018 | Train/Val Loss: 0.24\n",
      "Epoch: 009/100 | Batch 003/018 | Train/Val Loss: 0.22\n",
      "Epoch: 009/100 | Batch 004/018 | Train/Val Loss: 0.24\n",
      "Epoch: 009/100 | Batch 005/018 | Train/Val Loss: 0.24\n",
      "Epoch: 009/100 | Batch 006/018 | Train/Val Loss: 0.24\n",
      "Epoch: 009/100 | Batch 007/018 | Train/Val Loss: 0.22\n",
      "Epoch: 009/100 | Batch 008/018 | Train/Val Loss: 0.21\n",
      "Epoch: 009/100 | Batch 009/018 | Train/Val Loss: 0.22\n",
      "Epoch: 009/100 | Batch 010/018 | Train/Val Loss: 0.18\n",
      "Epoch: 009/100 | Batch 011/018 | Train/Val Loss: 0.24\n",
      "Epoch: 009/100 | Batch 012/018 | Train/Val Loss: 0.18\n",
      "Epoch: 009/100 | Batch 013/018 | Train/Val Loss: 0.19\n",
      "Epoch: 009/100 | Batch 014/018 | Train/Val Loss: 0.20\n",
      "Epoch: 009/100 | Batch 015/018 | Train/Val Loss: 0.22\n",
      "Epoch: 009/100 | Batch 016/018 | Train/Val Loss: 0.21\n",
      "Epoch: 009/100 | Batch 017/018 | Train/Val Loss: 0.22\n",
      "Epoch: 009/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 010/100 | Batch 000/018 | Train/Val Loss: 0.19\n",
      "Epoch: 010/100 | Batch 001/018 | Train/Val Loss: 0.16\n",
      "Epoch: 010/100 | Batch 002/018 | Train/Val Loss: 0.19\n",
      "Epoch: 010/100 | Batch 003/018 | Train/Val Loss: 0.27\n",
      "Epoch: 010/100 | Batch 004/018 | Train/Val Loss: 0.17\n",
      "Epoch: 010/100 | Batch 005/018 | Train/Val Loss: 0.19\n",
      "Epoch: 010/100 | Batch 006/018 | Train/Val Loss: 0.17\n",
      "Epoch: 010/100 | Batch 007/018 | Train/Val Loss: 0.17\n",
      "Epoch: 010/100 | Batch 008/018 | Train/Val Loss: 0.19\n",
      "Epoch: 010/100 | Batch 009/018 | Train/Val Loss: 0.15\n",
      "Epoch: 010/100 | Batch 010/018 | Train/Val Loss: 0.16\n",
      "Epoch: 010/100 | Batch 011/018 | Train/Val Loss: 0.14\n",
      "Epoch: 010/100 | Batch 012/018 | Train/Val Loss: 0.17\n",
      "Epoch: 010/100 | Batch 013/018 | Train/Val Loss: 0.12\n",
      "Epoch: 010/100 | Batch 014/018 | Train/Val Loss: 0.16\n",
      "Epoch: 010/100 | Batch 015/018 | Train/Val Loss: 0.19\n",
      "Epoch: 010/100 | Batch 016/018 | Train/Val Loss: 0.18\n",
      "Epoch: 010/100 | Batch 017/018 | Train/Val Loss: 0.20\n",
      "Epoch: 010/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 011/100 | Batch 000/018 | Train/Val Loss: 0.18\n",
      "Epoch: 011/100 | Batch 001/018 | Train/Val Loss: 0.12\n",
      "Epoch: 011/100 | Batch 002/018 | Train/Val Loss: 0.14\n",
      "Epoch: 011/100 | Batch 003/018 | Train/Val Loss: 0.15\n",
      "Epoch: 011/100 | Batch 004/018 | Train/Val Loss: 0.13\n",
      "Epoch: 011/100 | Batch 005/018 | Train/Val Loss: 0.15\n",
      "Epoch: 011/100 | Batch 006/018 | Train/Val Loss: 0.15\n",
      "Epoch: 011/100 | Batch 007/018 | Train/Val Loss: 0.17\n",
      "Epoch: 011/100 | Batch 008/018 | Train/Val Loss: 0.13\n",
      "Epoch: 011/100 | Batch 009/018 | Train/Val Loss: 0.15\n",
      "Epoch: 011/100 | Batch 010/018 | Train/Val Loss: 0.14\n",
      "Epoch: 011/100 | Batch 011/018 | Train/Val Loss: 0.18\n",
      "Epoch: 011/100 | Batch 012/018 | Train/Val Loss: 0.15\n",
      "Epoch: 011/100 | Batch 013/018 | Train/Val Loss: 0.18\n",
      "Epoch: 011/100 | Batch 014/018 | Train/Val Loss: 0.12\n",
      "Epoch: 011/100 | Batch 015/018 | Train/Val Loss: 0.14\n",
      "Epoch: 011/100 | Batch 016/018 | Train/Val Loss: 0.12\n",
      "Epoch: 011/100 | Batch 017/018 | Train/Val Loss: 0.08\n",
      "Epoch: 011/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 012/100 | Batch 000/018 | Train/Val Loss: 0.13\n",
      "Epoch: 012/100 | Batch 001/018 | Train/Val Loss: 0.13\n",
      "Epoch: 012/100 | Batch 002/018 | Train/Val Loss: 0.11\n",
      "Epoch: 012/100 | Batch 003/018 | Train/Val Loss: 0.18\n",
      "Epoch: 012/100 | Batch 004/018 | Train/Val Loss: 0.11\n",
      "Epoch: 012/100 | Batch 005/018 | Train/Val Loss: 0.14\n",
      "Epoch: 012/100 | Batch 006/018 | Train/Val Loss: 0.12\n",
      "Epoch: 012/100 | Batch 007/018 | Train/Val Loss: 0.11\n",
      "Epoch: 012/100 | Batch 008/018 | Train/Val Loss: 0.12\n",
      "Epoch: 012/100 | Batch 009/018 | Train/Val Loss: 0.12\n",
      "Epoch: 012/100 | Batch 010/018 | Train/Val Loss: 0.10\n",
      "Epoch: 012/100 | Batch 011/018 | Train/Val Loss: 0.15\n",
      "Epoch: 012/100 | Batch 012/018 | Train/Val Loss: 0.11\n",
      "Epoch: 012/100 | Batch 013/018 | Train/Val Loss: 0.09\n",
      "Epoch: 012/100 | Batch 014/018 | Train/Val Loss: 0.10\n",
      "Epoch: 012/100 | Batch 015/018 | Train/Val Loss: 0.08\n",
      "Epoch: 012/100 | Batch 016/018 | Train/Val Loss: 0.14\n",
      "Epoch: 012/100 | Batch 017/018 | Train/Val Loss: 0.13\n",
      "Epoch: 012/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 013/100 | Batch 000/018 | Train/Val Loss: 0.13\n",
      "Epoch: 013/100 | Batch 001/018 | Train/Val Loss: 0.14\n",
      "Epoch: 013/100 | Batch 002/018 | Train/Val Loss: 0.11\n",
      "Epoch: 013/100 | Batch 003/018 | Train/Val Loss: 0.11\n",
      "Epoch: 013/100 | Batch 004/018 | Train/Val Loss: 0.13\n",
      "Epoch: 013/100 | Batch 005/018 | Train/Val Loss: 0.10\n",
      "Epoch: 013/100 | Batch 006/018 | Train/Val Loss: 0.06\n",
      "Epoch: 013/100 | Batch 007/018 | Train/Val Loss: 0.10\n",
      "Epoch: 013/100 | Batch 008/018 | Train/Val Loss: 0.11\n",
      "Epoch: 013/100 | Batch 009/018 | Train/Val Loss: 0.07\n",
      "Epoch: 013/100 | Batch 010/018 | Train/Val Loss: 0.08\n",
      "Epoch: 013/100 | Batch 011/018 | Train/Val Loss: 0.08\n",
      "Epoch: 013/100 | Batch 012/018 | Train/Val Loss: 0.13\n",
      "Epoch: 013/100 | Batch 013/018 | Train/Val Loss: 0.14\n",
      "Epoch: 013/100 | Batch 014/018 | Train/Val Loss: 0.11\n",
      "Epoch: 013/100 | Batch 015/018 | Train/Val Loss: 0.06\n",
      "Epoch: 013/100 | Batch 016/018 | Train/Val Loss: 0.10\n",
      "Epoch: 013/100 | Batch 017/018 | Train/Val Loss: 0.08\n",
      "Epoch: 013/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 014/100 | Batch 000/018 | Train/Val Loss: 0.06\n",
      "Epoch: 014/100 | Batch 001/018 | Train/Val Loss: 0.07\n",
      "Epoch: 014/100 | Batch 002/018 | Train/Val Loss: 0.09\n",
      "Epoch: 014/100 | Batch 003/018 | Train/Val Loss: 0.11\n",
      "Epoch: 014/100 | Batch 004/018 | Train/Val Loss: 0.07\n",
      "Epoch: 014/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 014/100 | Batch 006/018 | Train/Val Loss: 0.09\n",
      "Epoch: 014/100 | Batch 007/018 | Train/Val Loss: 0.08\n",
      "Epoch: 014/100 | Batch 008/018 | Train/Val Loss: 0.11\n",
      "Epoch: 014/100 | Batch 009/018 | Train/Val Loss: 0.07\n",
      "Epoch: 014/100 | Batch 010/018 | Train/Val Loss: 0.10\n",
      "Epoch: 014/100 | Batch 011/018 | Train/Val Loss: 0.13\n",
      "Epoch: 014/100 | Batch 012/018 | Train/Val Loss: 0.09\n",
      "Epoch: 014/100 | Batch 013/018 | Train/Val Loss: 0.09\n",
      "Epoch: 014/100 | Batch 014/018 | Train/Val Loss: 0.10\n",
      "Epoch: 014/100 | Batch 015/018 | Train/Val Loss: 0.08\n",
      "Epoch: 014/100 | Batch 016/018 | Train/Val Loss: 0.06\n",
      "Epoch: 014/100 | Batch 017/018 | Train/Val Loss: 0.13\n",
      "Epoch: 014/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 015/100 | Batch 000/018 | Train/Val Loss: 0.12\n",
      "Epoch: 015/100 | Batch 001/018 | Train/Val Loss: 0.09\n",
      "Epoch: 015/100 | Batch 002/018 | Train/Val Loss: 0.07\n",
      "Epoch: 015/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 015/100 | Batch 004/018 | Train/Val Loss: 0.07\n",
      "Epoch: 015/100 | Batch 005/018 | Train/Val Loss: 0.07\n",
      "Epoch: 015/100 | Batch 006/018 | Train/Val Loss: 0.06\n",
      "Epoch: 015/100 | Batch 007/018 | Train/Val Loss: 0.06\n",
      "Epoch: 015/100 | Batch 008/018 | Train/Val Loss: 0.08\n",
      "Epoch: 015/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 015/100 | Batch 010/018 | Train/Val Loss: 0.08\n",
      "Epoch: 015/100 | Batch 011/018 | Train/Val Loss: 0.08\n",
      "Epoch: 015/100 | Batch 012/018 | Train/Val Loss: 0.09\n",
      "Epoch: 015/100 | Batch 013/018 | Train/Val Loss: 0.10\n",
      "Epoch: 015/100 | Batch 014/018 | Train/Val Loss: 0.06\n",
      "Epoch: 015/100 | Batch 015/018 | Train/Val Loss: 0.10\n",
      "Epoch: 015/100 | Batch 016/018 | Train/Val Loss: 0.11\n",
      "Epoch: 015/100 | Batch 017/018 | Train/Val Loss: 0.09\n",
      "Epoch: 015/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 016/100 | Batch 000/018 | Train/Val Loss: 0.07\n",
      "Epoch: 016/100 | Batch 001/018 | Train/Val Loss: 0.06\n",
      "Epoch: 016/100 | Batch 002/018 | Train/Val Loss: 0.06\n",
      "Epoch: 016/100 | Batch 003/018 | Train/Val Loss: 0.10\n",
      "Epoch: 016/100 | Batch 004/018 | Train/Val Loss: 0.07\n",
      "Epoch: 016/100 | Batch 005/018 | Train/Val Loss: 0.07\n",
      "Epoch: 016/100 | Batch 006/018 | Train/Val Loss: 0.07\n",
      "Epoch: 016/100 | Batch 007/018 | Train/Val Loss: 0.11\n",
      "Epoch: 016/100 | Batch 008/018 | Train/Val Loss: 0.10\n",
      "Epoch: 016/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 016/100 | Batch 010/018 | Train/Val Loss: 0.10\n",
      "Epoch: 016/100 | Batch 011/018 | Train/Val Loss: 0.08\n",
      "Epoch: 016/100 | Batch 012/018 | Train/Val Loss: 0.07\n",
      "Epoch: 016/100 | Batch 013/018 | Train/Val Loss: 0.07\n",
      "Epoch: 016/100 | Batch 014/018 | Train/Val Loss: 0.07\n",
      "Epoch: 016/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 016/100 | Batch 016/018 | Train/Val Loss: 0.06\n",
      "Epoch: 016/100 | Batch 017/018 | Train/Val Loss: 0.04\n",
      "Epoch: 016/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 017/100 | Batch 000/018 | Train/Val Loss: 0.12\n",
      "Epoch: 017/100 | Batch 001/018 | Train/Val Loss: 0.06\n",
      "Epoch: 017/100 | Batch 002/018 | Train/Val Loss: 0.08\n",
      "Epoch: 017/100 | Batch 003/018 | Train/Val Loss: 0.09\n",
      "Epoch: 017/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 017/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 017/100 | Batch 006/018 | Train/Val Loss: 0.06\n",
      "Epoch: 017/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 017/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 017/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 017/100 | Batch 010/018 | Train/Val Loss: 0.06\n",
      "Epoch: 017/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 017/100 | Batch 012/018 | Train/Val Loss: 0.06\n",
      "Epoch: 017/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 017/100 | Batch 014/018 | Train/Val Loss: 0.08\n",
      "Epoch: 017/100 | Batch 015/018 | Train/Val Loss: 0.05\n",
      "Epoch: 017/100 | Batch 016/018 | Train/Val Loss: 0.08\n",
      "Epoch: 017/100 | Batch 017/018 | Train/Val Loss: 0.10\n",
      "Epoch: 017/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 018/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 018/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 018/100 | Batch 002/018 | Train/Val Loss: 0.14\n",
      "Epoch: 018/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 018/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 018/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 018/100 | Batch 006/018 | Train/Val Loss: 0.08\n",
      "Epoch: 018/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 018/100 | Batch 008/018 | Train/Val Loss: 0.08\n",
      "Epoch: 018/100 | Batch 009/018 | Train/Val Loss: 0.07\n",
      "Epoch: 018/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 018/100 | Batch 011/018 | Train/Val Loss: 0.07\n",
      "Epoch: 018/100 | Batch 012/018 | Train/Val Loss: 0.07\n",
      "Epoch: 018/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 018/100 | Batch 014/018 | Train/Val Loss: 0.07\n",
      "Epoch: 018/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 018/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 018/100 | Batch 017/018 | Train/Val Loss: 0.05\n",
      "Epoch: 018/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 019/100 | Batch 000/018 | Train/Val Loss: 0.07\n",
      "Epoch: 019/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 019/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 019/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 019/100 | Batch 004/018 | Train/Val Loss: 0.07\n",
      "Epoch: 019/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 019/100 | Batch 006/018 | Train/Val Loss: 0.09\n",
      "Epoch: 019/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 019/100 | Batch 008/018 | Train/Val Loss: 0.05\n",
      "Epoch: 019/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 019/100 | Batch 010/018 | Train/Val Loss: 0.03\n",
      "Epoch: 019/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 019/100 | Batch 012/018 | Train/Val Loss: 0.10\n",
      "Epoch: 019/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 019/100 | Batch 014/018 | Train/Val Loss: 0.04\n",
      "Epoch: 019/100 | Batch 015/018 | Train/Val Loss: 0.08\n",
      "Epoch: 019/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 019/100 | Batch 017/018 | Train/Val Loss: 0.06\n",
      "Epoch: 019/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 020/100 | Batch 000/018 | Train/Val Loss: 0.06\n",
      "Epoch: 020/100 | Batch 001/018 | Train/Val Loss: 0.08\n",
      "Epoch: 020/100 | Batch 002/018 | Train/Val Loss: 0.11\n",
      "Epoch: 020/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 020/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 020/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 020/100 | Batch 006/018 | Train/Val Loss: 0.03\n",
      "Epoch: 020/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 020/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 020/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 020/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 020/100 | Batch 011/018 | Train/Val Loss: 0.06\n",
      "Epoch: 020/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 020/100 | Batch 013/018 | Train/Val Loss: 0.12\n",
      "Epoch: 020/100 | Batch 014/018 | Train/Val Loss: 0.04\n",
      "Epoch: 020/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 020/100 | Batch 016/018 | Train/Val Loss: 0.03\n",
      "Epoch: 020/100 | Batch 017/018 | Train/Val Loss: 0.04\n",
      "Epoch: 020/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 021/100 | Batch 000/018 | Train/Val Loss: 0.09\n",
      "Epoch: 021/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 021/100 | Batch 002/018 | Train/Val Loss: 0.08\n",
      "Epoch: 021/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 021/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 021/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 021/100 | Batch 006/018 | Train/Val Loss: 0.05\n",
      "Epoch: 021/100 | Batch 007/018 | Train/Val Loss: 0.02\n",
      "Epoch: 021/100 | Batch 008/018 | Train/Val Loss: 0.05\n",
      "Epoch: 021/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 021/100 | Batch 010/018 | Train/Val Loss: 0.03\n",
      "Epoch: 021/100 | Batch 011/018 | Train/Val Loss: 0.06\n",
      "Epoch: 021/100 | Batch 012/018 | Train/Val Loss: 0.04\n",
      "Epoch: 021/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 021/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 021/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 021/100 | Batch 016/018 | Train/Val Loss: 0.09\n",
      "Epoch: 021/100 | Batch 017/018 | Train/Val Loss: 0.06\n",
      "Epoch: 021/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 022/100 | Batch 000/018 | Train/Val Loss: 0.07\n",
      "Epoch: 022/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 022/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 022/100 | Batch 003/018 | Train/Val Loss: 0.09\n",
      "Epoch: 022/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 022/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 022/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 022/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 022/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 022/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 022/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 022/100 | Batch 011/018 | Train/Val Loss: 0.07\n",
      "Epoch: 022/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 022/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 022/100 | Batch 014/018 | Train/Val Loss: 0.05\n",
      "Epoch: 022/100 | Batch 015/018 | Train/Val Loss: 0.06\n",
      "Epoch: 022/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 022/100 | Batch 017/018 | Train/Val Loss: 0.08\n",
      "Epoch: 022/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 023/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 023/100 | Batch 001/018 | Train/Val Loss: 0.03\n",
      "Epoch: 023/100 | Batch 002/018 | Train/Val Loss: 0.06\n",
      "Epoch: 023/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 023/100 | Batch 004/018 | Train/Val Loss: 0.10\n",
      "Epoch: 023/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 023/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 023/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 023/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 023/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 023/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 023/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 023/100 | Batch 012/018 | Train/Val Loss: 0.06\n",
      "Epoch: 023/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 023/100 | Batch 014/018 | Train/Val Loss: 0.04\n",
      "Epoch: 023/100 | Batch 015/018 | Train/Val Loss: 0.05\n",
      "Epoch: 023/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 023/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 023/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 024/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 024/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 024/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 024/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 024/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 024/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 024/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 024/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 024/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 024/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 024/100 | Batch 010/018 | Train/Val Loss: 0.03\n",
      "Epoch: 024/100 | Batch 011/018 | Train/Val Loss: 0.09\n",
      "Epoch: 024/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 024/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 024/100 | Batch 014/018 | Train/Val Loss: 0.04\n",
      "Epoch: 024/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 024/100 | Batch 016/018 | Train/Val Loss: 0.07\n",
      "Epoch: 024/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 024/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 025/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 025/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 025/100 | Batch 002/018 | Train/Val Loss: 0.08\n",
      "Epoch: 025/100 | Batch 003/018 | Train/Val Loss: 0.07\n",
      "Epoch: 025/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 025/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 025/100 | Batch 006/018 | Train/Val Loss: 0.07\n",
      "Epoch: 025/100 | Batch 007/018 | Train/Val Loss: 0.03\n",
      "Epoch: 025/100 | Batch 008/018 | Train/Val Loss: 0.07\n",
      "Epoch: 025/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 025/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 025/100 | Batch 011/018 | Train/Val Loss: 0.02\n",
      "Epoch: 025/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 025/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 025/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 025/100 | Batch 015/018 | Train/Val Loss: 0.07\n",
      "Epoch: 025/100 | Batch 016/018 | Train/Val Loss: 0.05\n",
      "Epoch: 025/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 025/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 026/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 026/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 026/100 | Batch 002/018 | Train/Val Loss: 0.02\n",
      "Epoch: 026/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 026/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 026/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 026/100 | Batch 006/018 | Train/Val Loss: 0.04\n",
      "Epoch: 026/100 | Batch 007/018 | Train/Val Loss: 0.03\n",
      "Epoch: 026/100 | Batch 008/018 | Train/Val Loss: 0.09\n",
      "Epoch: 026/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 026/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 026/100 | Batch 011/018 | Train/Val Loss: 0.09\n",
      "Epoch: 026/100 | Batch 012/018 | Train/Val Loss: 0.04\n",
      "Epoch: 026/100 | Batch 013/018 | Train/Val Loss: 0.09\n",
      "Epoch: 026/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 026/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 026/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 026/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 026/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 027/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 027/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 027/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 027/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 027/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 027/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 027/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 027/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 027/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 027/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 027/100 | Batch 010/018 | Train/Val Loss: 0.06\n",
      "Epoch: 027/100 | Batch 011/018 | Train/Val Loss: 0.02\n",
      "Epoch: 027/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 027/100 | Batch 013/018 | Train/Val Loss: 0.06\n",
      "Epoch: 027/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 027/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 027/100 | Batch 016/018 | Train/Val Loss: 0.03\n",
      "Epoch: 027/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 027/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 028/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 028/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 028/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 028/100 | Batch 003/018 | Train/Val Loss: 0.09\n",
      "Epoch: 028/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 028/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 028/100 | Batch 006/018 | Train/Val Loss: 0.04\n",
      "Epoch: 028/100 | Batch 007/018 | Train/Val Loss: 0.02\n",
      "Epoch: 028/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 028/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 028/100 | Batch 010/018 | Train/Val Loss: 0.08\n",
      "Epoch: 028/100 | Batch 011/018 | Train/Val Loss: 0.02\n",
      "Epoch: 028/100 | Batch 012/018 | Train/Val Loss: 0.04\n",
      "Epoch: 028/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 028/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 028/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 028/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 028/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 028/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 029/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 029/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 029/100 | Batch 002/018 | Train/Val Loss: 0.06\n",
      "Epoch: 029/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 029/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 029/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 029/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 029/100 | Batch 007/018 | Train/Val Loss: 0.06\n",
      "Epoch: 029/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 029/100 | Batch 009/018 | Train/Val Loss: 0.04\n",
      "Epoch: 029/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 029/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 029/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 029/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 029/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 029/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 029/100 | Batch 016/018 | Train/Val Loss: 0.05\n",
      "Epoch: 029/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 029/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 030/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 030/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 030/100 | Batch 002/018 | Train/Val Loss: 0.06\n",
      "Epoch: 030/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 030/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 030/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 030/100 | Batch 006/018 | Train/Val Loss: 0.07\n",
      "Epoch: 030/100 | Batch 007/018 | Train/Val Loss: 0.02\n",
      "Epoch: 030/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 030/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 030/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 030/100 | Batch 011/018 | Train/Val Loss: 0.07\n",
      "Epoch: 030/100 | Batch 012/018 | Train/Val Loss: 0.03\n",
      "Epoch: 030/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 030/100 | Batch 014/018 | Train/Val Loss: 0.08\n",
      "Epoch: 030/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 030/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 030/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 030/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 031/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 031/100 | Batch 001/018 | Train/Val Loss: 0.06\n",
      "Epoch: 031/100 | Batch 002/018 | Train/Val Loss: 0.05\n",
      "Epoch: 031/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 031/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 031/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 031/100 | Batch 006/018 | Train/Val Loss: 0.03\n",
      "Epoch: 031/100 | Batch 007/018 | Train/Val Loss: 0.02\n",
      "Epoch: 031/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 031/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 031/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 031/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 031/100 | Batch 012/018 | Train/Val Loss: 0.06\n",
      "Epoch: 031/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 031/100 | Batch 014/018 | Train/Val Loss: 0.05\n",
      "Epoch: 031/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 031/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 031/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 031/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 032/100 | Batch 000/018 | Train/Val Loss: 0.07\n",
      "Epoch: 032/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 032/100 | Batch 002/018 | Train/Val Loss: 0.05\n",
      "Epoch: 032/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 032/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 032/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 032/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 032/100 | Batch 007/018 | Train/Val Loss: 0.06\n",
      "Epoch: 032/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 032/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 032/100 | Batch 010/018 | Train/Val Loss: 0.05\n",
      "Epoch: 032/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 032/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 032/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 032/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 032/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 032/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 032/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 032/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 033/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 033/100 | Batch 001/018 | Train/Val Loss: 0.06\n",
      "Epoch: 033/100 | Batch 002/018 | Train/Val Loss: 0.05\n",
      "Epoch: 033/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 033/100 | Batch 004/018 | Train/Val Loss: 0.06\n",
      "Epoch: 033/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 033/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 033/100 | Batch 007/018 | Train/Val Loss: 0.06\n",
      "Epoch: 033/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 033/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 033/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 033/100 | Batch 011/018 | Train/Val Loss: 0.02\n",
      "Epoch: 033/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 033/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 033/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 033/100 | Batch 015/018 | Train/Val Loss: 0.05\n",
      "Epoch: 033/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 033/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 033/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 034/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 034/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 034/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 034/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 034/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 034/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 034/100 | Batch 006/018 | Train/Val Loss: 0.05\n",
      "Epoch: 034/100 | Batch 007/018 | Train/Val Loss: 0.02\n",
      "Epoch: 034/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 034/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 034/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 034/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 034/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 034/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 034/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 034/100 | Batch 015/018 | Train/Val Loss: 0.07\n",
      "Epoch: 034/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 034/100 | Batch 017/018 | Train/Val Loss: 0.04\n",
      "Epoch: 034/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 035/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 035/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 035/100 | Batch 002/018 | Train/Val Loss: 0.06\n",
      "Epoch: 035/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 035/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 035/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 035/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 035/100 | Batch 007/018 | Train/Val Loss: 0.02\n",
      "Epoch: 035/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 035/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 035/100 | Batch 010/018 | Train/Val Loss: 0.07\n",
      "Epoch: 035/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 035/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 035/100 | Batch 013/018 | Train/Val Loss: 0.10\n",
      "Epoch: 035/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 035/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 035/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 035/100 | Batch 017/018 | Train/Val Loss: 0.04\n",
      "Epoch: 035/100 Train Acc: 0.986 Val Acc: 0.984\n",
      "Epoch: 036/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 036/100 | Batch 001/018 | Train/Val Loss: 0.08\n",
      "Epoch: 036/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 036/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 036/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 036/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 036/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 036/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 036/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 036/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 036/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 036/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 036/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 036/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 036/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 036/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 036/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 036/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 036/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 037/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 037/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 037/100 | Batch 002/018 | Train/Val Loss: 0.02\n",
      "Epoch: 037/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 037/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 037/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 037/100 | Batch 006/018 | Train/Val Loss: 0.04\n",
      "Epoch: 037/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 037/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 037/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 037/100 | Batch 010/018 | Train/Val Loss: 0.07\n",
      "Epoch: 037/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 037/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 037/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 037/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 037/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 037/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 037/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 037/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 038/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 038/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 038/100 | Batch 002/018 | Train/Val Loss: 0.02\n",
      "Epoch: 038/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 038/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 038/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 038/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 038/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 038/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 038/100 | Batch 009/018 | Train/Val Loss: 0.04\n",
      "Epoch: 038/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 038/100 | Batch 011/018 | Train/Val Loss: 0.08\n",
      "Epoch: 038/100 | Batch 012/018 | Train/Val Loss: 0.03\n",
      "Epoch: 038/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 038/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 038/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 038/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 038/100 | Batch 017/018 | Train/Val Loss: 0.07\n",
      "Epoch: 038/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 039/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 039/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 039/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 039/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 039/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 039/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 039/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 039/100 | Batch 007/018 | Train/Val Loss: 0.06\n",
      "Epoch: 039/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 039/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 039/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 039/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 039/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 039/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 039/100 | Batch 014/018 | Train/Val Loss: 0.06\n",
      "Epoch: 039/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 039/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 039/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 039/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 040/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 040/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 040/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 040/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 040/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 040/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 040/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 040/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 040/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 040/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 040/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 040/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 040/100 | Batch 012/018 | Train/Val Loss: 0.06\n",
      "Epoch: 040/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 040/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 040/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 040/100 | Batch 016/018 | Train/Val Loss: 0.05\n",
      "Epoch: 040/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 040/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 041/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 041/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 041/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 041/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 041/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 041/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 041/100 | Batch 006/018 | Train/Val Loss: 0.05\n",
      "Epoch: 041/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 041/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 041/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 041/100 | Batch 010/018 | Train/Val Loss: 0.05\n",
      "Epoch: 041/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 041/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 041/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 041/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 041/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 041/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 041/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 041/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 042/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 042/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 042/100 | Batch 002/018 | Train/Val Loss: 0.02\n",
      "Epoch: 042/100 | Batch 003/018 | Train/Val Loss: 0.04\n",
      "Epoch: 042/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 042/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 042/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 042/100 | Batch 007/018 | Train/Val Loss: 0.03\n",
      "Epoch: 042/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 042/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 042/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 042/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 042/100 | Batch 012/018 | Train/Val Loss: 0.06\n",
      "Epoch: 042/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 042/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 042/100 | Batch 015/018 | Train/Val Loss: 0.07\n",
      "Epoch: 042/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 042/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 042/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 043/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 043/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 002/018 | Train/Val Loss: 0.06\n",
      "Epoch: 043/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 043/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 043/100 | Batch 006/018 | Train/Val Loss: 0.08\n",
      "Epoch: 043/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 043/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 010/018 | Train/Val Loss: 0.03\n",
      "Epoch: 043/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 043/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 043/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 043/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 044/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 044/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 044/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 044/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 044/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 044/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 044/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 044/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 044/100 | Batch 008/018 | Train/Val Loss: 0.07\n",
      "Epoch: 044/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 044/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 044/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 044/100 | Batch 012/018 | Train/Val Loss: 0.04\n",
      "Epoch: 044/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 044/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 044/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 044/100 | Batch 016/018 | Train/Val Loss: 0.03\n",
      "Epoch: 044/100 | Batch 017/018 | Train/Val Loss: 0.04\n",
      "Epoch: 044/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 045/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 045/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 045/100 | Batch 002/018 | Train/Val Loss: 0.05\n",
      "Epoch: 045/100 | Batch 003/018 | Train/Val Loss: 0.04\n",
      "Epoch: 045/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 045/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 045/100 | Batch 006/018 | Train/Val Loss: 0.04\n",
      "Epoch: 045/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 045/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 045/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 045/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 045/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 045/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 045/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 045/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 045/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 045/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 045/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 045/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 046/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 046/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 046/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 046/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 046/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 046/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 046/100 | Batch 014/018 | Train/Val Loss: 0.07\n",
      "Epoch: 046/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 046/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 046/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 046/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 047/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 047/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 047/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 047/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 047/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 047/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 047/100 | Batch 006/018 | Train/Val Loss: 0.08\n",
      "Epoch: 047/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 047/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 047/100 | Batch 009/018 | Train/Val Loss: 0.07\n",
      "Epoch: 047/100 | Batch 010/018 | Train/Val Loss: 0.03\n",
      "Epoch: 047/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 047/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 047/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 047/100 | Batch 014/018 | Train/Val Loss: 0.05\n",
      "Epoch: 047/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 047/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 047/100 | Batch 017/018 | Train/Val Loss: 0.05\n",
      "Epoch: 047/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 048/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 048/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 048/100 | Batch 004/018 | Train/Val Loss: 0.08\n",
      "Epoch: 048/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 048/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 048/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 048/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 048/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 048/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 048/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 048/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 048/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 049/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 049/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 049/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 049/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 049/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 049/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 049/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 049/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 049/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 049/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 049/100 | Batch 010/018 | Train/Val Loss: 0.06\n",
      "Epoch: 049/100 | Batch 011/018 | Train/Val Loss: 0.00\n",
      "Epoch: 049/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 049/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 049/100 | Batch 014/018 | Train/Val Loss: 0.05\n",
      "Epoch: 049/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 049/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 049/100 | Batch 017/018 | Train/Val Loss: 0.08\n",
      "Epoch: 049/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 050/100 | Batch 000/018 | Train/Val Loss: 0.06\n",
      "Epoch: 050/100 | Batch 001/018 | Train/Val Loss: 0.03\n",
      "Epoch: 050/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 004/018 | Train/Val Loss: 0.07\n",
      "Epoch: 050/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 050/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 050/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 050/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 050/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 050/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 050/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 050/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 051/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 051/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 051/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 051/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 051/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 051/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 051/100 | Batch 006/018 | Train/Val Loss: 0.04\n",
      "Epoch: 051/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 051/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 051/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 051/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 051/100 | Batch 011/018 | Train/Val Loss: 0.08\n",
      "Epoch: 051/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 051/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 051/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 051/100 | Batch 015/018 | Train/Val Loss: 0.07\n",
      "Epoch: 051/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 051/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 051/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 052/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 052/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 052/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 052/100 | Batch 003/018 | Train/Val Loss: 0.04\n",
      "Epoch: 052/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 052/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 052/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 052/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 052/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 052/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 052/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 052/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 052/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 052/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 052/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 052/100 | Batch 015/018 | Train/Val Loss: 0.05\n",
      "Epoch: 052/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 052/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 052/100 Train Acc: 0.988 Val Acc: 0.984\n",
      "Epoch: 053/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 053/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 053/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 053/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 053/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 053/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 053/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 053/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 053/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 053/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 053/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 053/100 | Batch 011/018 | Train/Val Loss: 0.02\n",
      "Epoch: 053/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 053/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 053/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 053/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 053/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 053/100 | Batch 017/018 | Train/Val Loss: 0.06\n",
      "Epoch: 053/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 054/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 054/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 054/100 | Batch 002/018 | Train/Val Loss: 0.05\n",
      "Epoch: 054/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 054/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 054/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 054/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 054/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 054/100 | Batch 008/018 | Train/Val Loss: 0.05\n",
      "Epoch: 054/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 054/100 | Batch 010/018 | Train/Val Loss: 0.05\n",
      "Epoch: 054/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 054/100 | Batch 012/018 | Train/Val Loss: 0.04\n",
      "Epoch: 054/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 054/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 054/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 054/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 054/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 054/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 055/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 007/018 | Train/Val Loss: 0.07\n",
      "Epoch: 055/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 009/018 | Train/Val Loss: 0.04\n",
      "Epoch: 055/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 055/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 055/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 055/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 055/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 055/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 055/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 055/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 055/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 056/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 056/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 056/100 | Batch 002/018 | Train/Val Loss: 0.02\n",
      "Epoch: 056/100 | Batch 003/018 | Train/Val Loss: 0.04\n",
      "Epoch: 056/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 056/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 056/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 056/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 056/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 056/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 056/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 056/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 056/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 056/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 056/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 056/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 056/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 056/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 056/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 057/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 057/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 057/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 057/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 057/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 057/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 057/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 057/100 | Batch 007/018 | Train/Val Loss: 0.08\n",
      "Epoch: 057/100 | Batch 008/018 | Train/Val Loss: 0.05\n",
      "Epoch: 057/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 057/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 057/100 | Batch 011/018 | Train/Val Loss: 0.06\n",
      "Epoch: 057/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 057/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 057/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 057/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 057/100 | Batch 016/018 | Train/Val Loss: 0.07\n",
      "Epoch: 057/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 057/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 058/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 058/100 | Batch 001/018 | Train/Val Loss: 0.06\n",
      "Epoch: 058/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 058/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 058/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 058/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 058/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 058/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 058/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 058/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 058/100 | Batch 010/018 | Train/Val Loss: 0.08\n",
      "Epoch: 058/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 058/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 058/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 058/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 058/100 | Batch 015/018 | Train/Val Loss: 0.05\n",
      "Epoch: 058/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 058/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 058/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 059/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 059/100 | Batch 001/018 | Train/Val Loss: 0.03\n",
      "Epoch: 059/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 059/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 059/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 059/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 | Batch 008/018 | Train/Val Loss: 0.05\n",
      "Epoch: 059/100 | Batch 009/018 | Train/Val Loss: 0.04\n",
      "Epoch: 059/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 059/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 | Batch 014/018 | Train/Val Loss: 0.05\n",
      "Epoch: 059/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 | Batch 016/018 | Train/Val Loss: 0.05\n",
      "Epoch: 059/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 059/100 Train Acc: 0.990 Val Acc: 0.984\n",
      "Epoch: 060/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 060/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 060/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 060/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 060/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 060/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 060/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 060/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 060/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 060/100 | Batch 009/018 | Train/Val Loss: 0.10\n",
      "Epoch: 060/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 060/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 060/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 060/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 060/100 | Batch 014/018 | Train/Val Loss: 0.05\n",
      "Epoch: 060/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 060/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 060/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 060/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 061/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 061/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 061/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 061/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 061/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 061/100 | Batch 005/018 | Train/Val Loss: 0.08\n",
      "Epoch: 061/100 | Batch 006/018 | Train/Val Loss: 0.03\n",
      "Epoch: 061/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 061/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 061/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 061/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 061/100 | Batch 011/018 | Train/Val Loss: 0.06\n",
      "Epoch: 061/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 061/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 061/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 061/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 061/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 061/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 061/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 062/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 062/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 062/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 062/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 062/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 062/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 062/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 062/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 062/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 062/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 062/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 062/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 062/100 | Batch 012/018 | Train/Val Loss: 0.03\n",
      "Epoch: 062/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 062/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 062/100 | Batch 015/018 | Train/Val Loss: 0.08\n",
      "Epoch: 062/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 062/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 062/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 063/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 063/100 | Batch 001/018 | Train/Val Loss: 0.07\n",
      "Epoch: 063/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 063/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 063/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 063/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 063/100 | Batch 006/018 | Train/Val Loss: 0.03\n",
      "Epoch: 063/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 063/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 063/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 063/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 063/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 063/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 063/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 063/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 063/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 063/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 063/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 063/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 064/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 064/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 | Batch 003/018 | Train/Val Loss: 0.06\n",
      "Epoch: 064/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 064/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 064/100 | Batch 006/018 | Train/Val Loss: 0.07\n",
      "Epoch: 064/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 064/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 064/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 064/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 064/100 | Batch 016/018 | Train/Val Loss: 0.03\n",
      "Epoch: 064/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 064/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 065/100 | Batch 000/018 | Train/Val Loss: 0.07\n",
      "Epoch: 065/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 065/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 065/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 065/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 065/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 065/100 | Batch 006/018 | Train/Val Loss: 0.07\n",
      "Epoch: 065/100 | Batch 007/018 | Train/Val Loss: 0.03\n",
      "Epoch: 065/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 065/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 065/100 | Batch 010/018 | Train/Val Loss: 0.03\n",
      "Epoch: 065/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 065/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 065/100 | Batch 013/018 | Train/Val Loss: 0.02\n",
      "Epoch: 065/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 065/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 065/100 | Batch 016/018 | Train/Val Loss: 0.03\n",
      "Epoch: 065/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 065/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 066/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 066/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 066/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 066/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 007/018 | Train/Val Loss: 0.06\n",
      "Epoch: 066/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 066/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 012/018 | Train/Val Loss: 0.03\n",
      "Epoch: 066/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 014/018 | Train/Val Loss: 0.12\n",
      "Epoch: 066/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 066/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 067/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 067/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 067/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 067/100 | Batch 003/018 | Train/Val Loss: 0.04\n",
      "Epoch: 067/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 067/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 067/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 067/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 067/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 067/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 067/100 | Batch 010/018 | Train/Val Loss: 0.06\n",
      "Epoch: 067/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 067/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 067/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 067/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 067/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 067/100 | Batch 016/018 | Train/Val Loss: 0.03\n",
      "Epoch: 067/100 | Batch 017/018 | Train/Val Loss: 0.05\n",
      "Epoch: 067/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 068/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 068/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 068/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 068/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 068/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 068/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 068/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 068/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 068/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 068/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 068/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 068/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 068/100 | Batch 012/018 | Train/Val Loss: 0.04\n",
      "Epoch: 068/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 068/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 068/100 | Batch 015/018 | Train/Val Loss: 0.05\n",
      "Epoch: 068/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 068/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 068/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 069/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 069/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 069/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 069/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 069/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 069/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 069/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 069/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 069/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 069/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 069/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 069/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 069/100 | Batch 012/018 | Train/Val Loss: 0.07\n",
      "Epoch: 069/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 069/100 | Batch 014/018 | Train/Val Loss: 0.03\n",
      "Epoch: 069/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 069/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 069/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 069/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 070/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 070/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 070/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 070/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 070/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 070/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 070/100 | Batch 006/018 | Train/Val Loss: 0.04\n",
      "Epoch: 070/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 070/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 070/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 070/100 | Batch 010/018 | Train/Val Loss: 0.07\n",
      "Epoch: 070/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 070/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 070/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 070/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 070/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 070/100 | Batch 016/018 | Train/Val Loss: 0.05\n",
      "Epoch: 070/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 070/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 071/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 071/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 002/018 | Train/Val Loss: 0.08\n",
      "Epoch: 071/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 071/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 006/018 | Train/Val Loss: 0.06\n",
      "Epoch: 071/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 071/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 071/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 071/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 071/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 071/100 | Batch 016/018 | Train/Val Loss: 0.03\n",
      "Epoch: 071/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 071/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 072/100 | Batch 000/018 | Train/Val Loss: 0.05\n",
      "Epoch: 072/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 072/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 072/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 072/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 072/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 072/100 | Batch 006/018 | Train/Val Loss: 0.04\n",
      "Epoch: 072/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 072/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 072/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 072/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 072/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 072/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 072/100 | Batch 013/018 | Train/Val Loss: 0.06\n",
      "Epoch: 072/100 | Batch 014/018 | Train/Val Loss: 0.09\n",
      "Epoch: 072/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 072/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 072/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 072/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 073/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 073/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 073/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 073/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 073/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 073/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 073/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 073/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 073/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 073/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 073/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 073/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 073/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 073/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 073/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 073/100 | Batch 015/018 | Train/Val Loss: 0.07\n",
      "Epoch: 073/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 073/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 073/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 074/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 074/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 074/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 074/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 074/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 074/100 | Batch 005/018 | Train/Val Loss: 0.05\n",
      "Epoch: 074/100 | Batch 006/018 | Train/Val Loss: 0.03\n",
      "Epoch: 074/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 074/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 074/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 074/100 | Batch 010/018 | Train/Val Loss: 0.06\n",
      "Epoch: 074/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 074/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 074/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 074/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 074/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 074/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 074/100 | Batch 017/018 | Train/Val Loss: 0.07\n",
      "Epoch: 074/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 075/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 075/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 075/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 075/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 075/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 075/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 075/100 | Batch 006/018 | Train/Val Loss: 0.06\n",
      "Epoch: 075/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 075/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 075/100 | Batch 009/018 | Train/Val Loss: 0.04\n",
      "Epoch: 075/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 075/100 | Batch 011/018 | Train/Val Loss: 0.00\n",
      "Epoch: 075/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 075/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 075/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 075/100 | Batch 015/018 | Train/Val Loss: 0.07\n",
      "Epoch: 075/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 075/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 075/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 076/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 076/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 076/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 076/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 076/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 076/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 076/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 076/100 | Batch 007/018 | Train/Val Loss: 0.03\n",
      "Epoch: 076/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 076/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 076/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 076/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 076/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 076/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 076/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 076/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 076/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 076/100 | Batch 017/018 | Train/Val Loss: 0.05\n",
      "Epoch: 076/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 077/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 077/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 077/100 | Batch 002/018 | Train/Val Loss: 0.02\n",
      "Epoch: 077/100 | Batch 003/018 | Train/Val Loss: 0.07\n",
      "Epoch: 077/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 077/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 077/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 077/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 077/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 077/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 077/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 077/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 077/100 | Batch 012/018 | Train/Val Loss: 0.08\n",
      "Epoch: 077/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 077/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 077/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 077/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 077/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 077/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 078/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 078/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 078/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 078/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 078/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 078/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 078/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 078/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 078/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 078/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 078/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 078/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 078/100 | Batch 012/018 | Train/Val Loss: 0.04\n",
      "Epoch: 078/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 078/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 078/100 | Batch 015/018 | Train/Val Loss: 0.08\n",
      "Epoch: 078/100 | Batch 016/018 | Train/Val Loss: 0.07\n",
      "Epoch: 078/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 078/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 079/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 079/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 079/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 079/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 079/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 079/100 | Batch 005/018 | Train/Val Loss: 0.02\n",
      "Epoch: 079/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 079/100 | Batch 007/018 | Train/Val Loss: 0.06\n",
      "Epoch: 079/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 079/100 | Batch 009/018 | Train/Val Loss: 0.04\n",
      "Epoch: 079/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 079/100 | Batch 011/018 | Train/Val Loss: 0.03\n",
      "Epoch: 079/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 079/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 079/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 079/100 | Batch 015/018 | Train/Val Loss: 0.04\n",
      "Epoch: 079/100 | Batch 016/018 | Train/Val Loss: 0.05\n",
      "Epoch: 079/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 079/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 080/100 | Batch 000/018 | Train/Val Loss: 0.08\n",
      "Epoch: 080/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 080/100 | Batch 002/018 | Train/Val Loss: 0.07\n",
      "Epoch: 080/100 | Batch 003/018 | Train/Val Loss: 0.04\n",
      "Epoch: 080/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 080/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 080/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 080/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 080/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 080/100 | Batch 009/018 | Train/Val Loss: 0.04\n",
      "Epoch: 080/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 080/100 | Batch 011/018 | Train/Val Loss: 0.00\n",
      "Epoch: 080/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 080/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 080/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 080/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 080/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 080/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 080/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 081/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 081/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 081/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 081/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 081/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 081/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 081/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 081/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 081/100 | Batch 008/018 | Train/Val Loss: 0.06\n",
      "Epoch: 081/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 081/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 081/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 081/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 081/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 081/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 081/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 081/100 | Batch 016/018 | Train/Val Loss: 0.02\n",
      "Epoch: 081/100 | Batch 017/018 | Train/Val Loss: 0.08\n",
      "Epoch: 081/100 Train Acc: 0.993 Val Acc: 0.984\n",
      "Epoch: 082/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 082/100 | Batch 001/018 | Train/Val Loss: 0.04\n",
      "Epoch: 082/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 082/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 082/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 082/100 | Batch 005/018 | Train/Val Loss: 0.06\n",
      "Epoch: 082/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 082/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 082/100 | Batch 008/018 | Train/Val Loss: 0.01\n",
      "Epoch: 082/100 | Batch 009/018 | Train/Val Loss: 0.06\n",
      "Epoch: 082/100 | Batch 010/018 | Train/Val Loss: 0.03\n",
      "Epoch: 082/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 082/100 | Batch 012/018 | Train/Val Loss: 0.03\n",
      "Epoch: 082/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 082/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 082/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 082/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 082/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 082/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 083/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 083/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 083/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 083/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 083/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 083/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 083/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 083/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 083/100 | Batch 008/018 | Train/Val Loss: 0.07\n",
      "Epoch: 083/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 083/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 083/100 | Batch 011/018 | Train/Val Loss: 0.00\n",
      "Epoch: 083/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 083/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 083/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 083/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 083/100 | Batch 016/018 | Train/Val Loss: 0.04\n",
      "Epoch: 083/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 083/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 084/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 084/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 084/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 084/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 084/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 084/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 084/100 | Batch 006/018 | Train/Val Loss: 0.05\n",
      "Epoch: 084/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 084/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 084/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 084/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 084/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 084/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 084/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 084/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 084/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 084/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 084/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 084/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 085/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 085/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 085/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 085/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 085/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 085/100 | Batch 005/018 | Train/Val Loss: 0.07\n",
      "Epoch: 085/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 085/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 085/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 085/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 085/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 085/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 085/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 085/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 085/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 085/100 | Batch 015/018 | Train/Val Loss: 0.06\n",
      "Epoch: 085/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 085/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 085/100 Train Acc: 0.993 Val Acc: 0.984\n",
      "Epoch: 086/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 086/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 086/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 086/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 086/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 086/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 086/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 086/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 086/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 086/100 | Batch 009/018 | Train/Val Loss: 0.08\n",
      "Epoch: 086/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 086/100 | Batch 011/018 | Train/Val Loss: 0.06\n",
      "Epoch: 086/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 086/100 | Batch 013/018 | Train/Val Loss: 0.07\n",
      "Epoch: 086/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 086/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 086/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 086/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 086/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 087/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 087/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 087/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 087/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 087/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 087/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 087/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 087/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 087/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 087/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 087/100 | Batch 010/018 | Train/Val Loss: 0.07\n",
      "Epoch: 087/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 087/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 087/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 087/100 | Batch 014/018 | Train/Val Loss: 0.04\n",
      "Epoch: 087/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 087/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 087/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 087/100 Train Acc: 0.993 Val Acc: 0.984\n",
      "Epoch: 088/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 088/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 088/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 088/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 088/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 088/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 088/100 | Batch 006/018 | Train/Val Loss: 0.05\n",
      "Epoch: 088/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 088/100 | Batch 008/018 | Train/Val Loss: 0.05\n",
      "Epoch: 088/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 088/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 088/100 | Batch 011/018 | Train/Val Loss: 0.00\n",
      "Epoch: 088/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 088/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 088/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 088/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 088/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 088/100 | Batch 017/018 | Train/Val Loss: 0.08\n",
      "Epoch: 088/100 Train Acc: 0.993 Val Acc: 0.984\n",
      "Epoch: 089/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 089/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 089/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 089/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 089/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 089/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 089/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 089/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 089/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 089/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 089/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 089/100 | Batch 011/018 | Train/Val Loss: 0.02\n",
      "Epoch: 089/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 089/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 089/100 | Batch 014/018 | Train/Val Loss: 0.04\n",
      "Epoch: 089/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 089/100 | Batch 016/018 | Train/Val Loss: 0.07\n",
      "Epoch: 089/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 089/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 090/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 090/100 | Batch 001/018 | Train/Val Loss: 0.05\n",
      "Epoch: 090/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 090/100 | Batch 003/018 | Train/Val Loss: 0.01\n",
      "Epoch: 090/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 090/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 090/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 090/100 | Batch 007/018 | Train/Val Loss: 0.02\n",
      "Epoch: 090/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 090/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 090/100 | Batch 010/018 | Train/Val Loss: 0.10\n",
      "Epoch: 090/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 090/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 090/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 090/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 090/100 | Batch 015/018 | Train/Val Loss: 0.05\n",
      "Epoch: 090/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 090/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 090/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 091/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 091/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 091/100 | Batch 002/018 | Train/Val Loss: 0.04\n",
      "Epoch: 091/100 | Batch 003/018 | Train/Val Loss: 0.04\n",
      "Epoch: 091/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 091/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 091/100 | Batch 006/018 | Train/Val Loss: 0.02\n",
      "Epoch: 091/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 091/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 091/100 | Batch 009/018 | Train/Val Loss: 0.03\n",
      "Epoch: 091/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 091/100 | Batch 011/018 | Train/Val Loss: 0.00\n",
      "Epoch: 091/100 | Batch 012/018 | Train/Val Loss: 0.02\n",
      "Epoch: 091/100 | Batch 013/018 | Train/Val Loss: 0.05\n",
      "Epoch: 091/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 091/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 091/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 091/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 091/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 092/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 092/100 | Batch 001/018 | Train/Val Loss: 0.03\n",
      "Epoch: 092/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 092/100 | Batch 003/018 | Train/Val Loss: 0.05\n",
      "Epoch: 092/100 | Batch 004/018 | Train/Val Loss: 0.01\n",
      "Epoch: 092/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 092/100 | Batch 006/018 | Train/Val Loss: 0.03\n",
      "Epoch: 092/100 | Batch 007/018 | Train/Val Loss: 0.05\n",
      "Epoch: 092/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 092/100 | Batch 009/018 | Train/Val Loss: 0.05\n",
      "Epoch: 092/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 092/100 | Batch 011/018 | Train/Val Loss: 0.02\n",
      "Epoch: 092/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 092/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 092/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 092/100 | Batch 015/018 | Train/Val Loss: 0.01\n",
      "Epoch: 092/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 092/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 092/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 093/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 093/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 093/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 093/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 093/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 093/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 093/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 093/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 093/100 | Batch 008/018 | Train/Val Loss: 0.03\n",
      "Epoch: 093/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 093/100 | Batch 010/018 | Train/Val Loss: 0.04\n",
      "Epoch: 093/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 093/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 093/100 | Batch 013/018 | Train/Val Loss: 0.03\n",
      "Epoch: 093/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 093/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 093/100 | Batch 016/018 | Train/Val Loss: 0.05\n",
      "Epoch: 093/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 093/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 094/100 | Batch 000/018 | Train/Val Loss: 0.02\n",
      "Epoch: 094/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 094/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 094/100 | Batch 003/018 | Train/Val Loss: 0.03\n",
      "Epoch: 094/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 094/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 094/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 094/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 094/100 | Batch 008/018 | Train/Val Loss: 0.02\n",
      "Epoch: 094/100 | Batch 009/018 | Train/Val Loss: 0.01\n",
      "Epoch: 094/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 094/100 | Batch 011/018 | Train/Val Loss: 0.05\n",
      "Epoch: 094/100 | Batch 012/018 | Train/Val Loss: 0.06\n",
      "Epoch: 094/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 094/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 094/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 094/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 094/100 | Batch 017/018 | Train/Val Loss: 0.00\n",
      "Epoch: 094/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 095/100 | Batch 000/018 | Train/Val Loss: 0.01\n",
      "Epoch: 095/100 | Batch 001/018 | Train/Val Loss: 0.02\n",
      "Epoch: 095/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 095/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 095/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 095/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 095/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 095/100 | Batch 007/018 | Train/Val Loss: 0.04\n",
      "Epoch: 095/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 095/100 | Batch 009/018 | Train/Val Loss: 0.09\n",
      "Epoch: 095/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 095/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 095/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 095/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 095/100 | Batch 014/018 | Train/Val Loss: 0.05\n",
      "Epoch: 095/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 095/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 095/100 | Batch 017/018 | Train/Val Loss: 0.02\n",
      "Epoch: 095/100 Train Acc: 0.993 Val Acc: 0.984\n",
      "Epoch: 096/100 | Batch 000/018 | Train/Val Loss: 0.03\n",
      "Epoch: 096/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 096/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 096/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 096/100 | Batch 004/018 | Train/Val Loss: 0.04\n",
      "Epoch: 096/100 | Batch 005/018 | Train/Val Loss: 0.03\n",
      "Epoch: 096/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 096/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 096/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 096/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 096/100 | Batch 010/018 | Train/Val Loss: 0.05\n",
      "Epoch: 096/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 096/100 | Batch 012/018 | Train/Val Loss: 0.05\n",
      "Epoch: 096/100 | Batch 013/018 | Train/Val Loss: 0.08\n",
      "Epoch: 096/100 | Batch 014/018 | Train/Val Loss: 0.02\n",
      "Epoch: 096/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 096/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 096/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 096/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 097/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 097/100 | Batch 001/018 | Train/Val Loss: 0.03\n",
      "Epoch: 097/100 | Batch 002/018 | Train/Val Loss: 0.02\n",
      "Epoch: 097/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 097/100 | Batch 004/018 | Train/Val Loss: 0.02\n",
      "Epoch: 097/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 097/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 097/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 097/100 | Batch 008/018 | Train/Val Loss: 0.05\n",
      "Epoch: 097/100 | Batch 009/018 | Train/Val Loss: 0.07\n",
      "Epoch: 097/100 | Batch 010/018 | Train/Val Loss: 0.05\n",
      "Epoch: 097/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 097/100 | Batch 012/018 | Train/Val Loss: 0.07\n",
      "Epoch: 097/100 | Batch 013/018 | Train/Val Loss: 0.01\n",
      "Epoch: 097/100 | Batch 014/018 | Train/Val Loss: 0.01\n",
      "Epoch: 097/100 | Batch 015/018 | Train/Val Loss: 0.00\n",
      "Epoch: 097/100 | Batch 016/018 | Train/Val Loss: 0.00\n",
      "Epoch: 097/100 | Batch 017/018 | Train/Val Loss: 0.01\n",
      "Epoch: 097/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 098/100 | Batch 000/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 001/018 | Train/Val Loss: 0.11\n",
      "Epoch: 098/100 | Batch 002/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 004/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 005/018 | Train/Val Loss: 0.04\n",
      "Epoch: 098/100 | Batch 006/018 | Train/Val Loss: 0.01\n",
      "Epoch: 098/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 098/100 | Batch 009/018 | Train/Val Loss: 0.02\n",
      "Epoch: 098/100 | Batch 010/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 011/018 | Train/Val Loss: 0.04\n",
      "Epoch: 098/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 098/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 098/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 098/100 | Batch 017/018 | Train/Val Loss: 0.03\n",
      "Epoch: 098/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 099/100 | Batch 000/018 | Train/Val Loss: 0.04\n",
      "Epoch: 099/100 | Batch 001/018 | Train/Val Loss: 0.00\n",
      "Epoch: 099/100 | Batch 002/018 | Train/Val Loss: 0.01\n",
      "Epoch: 099/100 | Batch 003/018 | Train/Val Loss: 0.02\n",
      "Epoch: 099/100 | Batch 004/018 | Train/Val Loss: 0.05\n",
      "Epoch: 099/100 | Batch 005/018 | Train/Val Loss: 0.01\n",
      "Epoch: 099/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 099/100 | Batch 007/018 | Train/Val Loss: 0.01\n",
      "Epoch: 099/100 | Batch 008/018 | Train/Val Loss: 0.04\n",
      "Epoch: 099/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 099/100 | Batch 010/018 | Train/Val Loss: 0.01\n",
      "Epoch: 099/100 | Batch 011/018 | Train/Val Loss: 0.07\n",
      "Epoch: 099/100 | Batch 012/018 | Train/Val Loss: 0.01\n",
      "Epoch: 099/100 | Batch 013/018 | Train/Val Loss: 0.00\n",
      "Epoch: 099/100 | Batch 014/018 | Train/Val Loss: 0.00\n",
      "Epoch: 099/100 | Batch 015/018 | Train/Val Loss: 0.03\n",
      "Epoch: 099/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 099/100 | Batch 017/018 | Train/Val Loss: 0.06\n",
      "Epoch: 099/100 Train Acc: 0.991 Val Acc: 0.984\n",
      "Epoch: 100/100 | Batch 000/018 | Train/Val Loss: 0.07\n",
      "Epoch: 100/100 | Batch 001/018 | Train/Val Loss: 0.01\n",
      "Epoch: 100/100 | Batch 002/018 | Train/Val Loss: 0.03\n",
      "Epoch: 100/100 | Batch 003/018 | Train/Val Loss: 0.00\n",
      "Epoch: 100/100 | Batch 004/018 | Train/Val Loss: 0.03\n",
      "Epoch: 100/100 | Batch 005/018 | Train/Val Loss: 0.00\n",
      "Epoch: 100/100 | Batch 006/018 | Train/Val Loss: 0.00\n",
      "Epoch: 100/100 | Batch 007/018 | Train/Val Loss: 0.00\n",
      "Epoch: 100/100 | Batch 008/018 | Train/Val Loss: 0.00\n",
      "Epoch: 100/100 | Batch 009/018 | Train/Val Loss: 0.00\n",
      "Epoch: 100/100 | Batch 010/018 | Train/Val Loss: 0.02\n",
      "Epoch: 100/100 | Batch 011/018 | Train/Val Loss: 0.01\n",
      "Epoch: 100/100 | Batch 012/018 | Train/Val Loss: 0.00\n",
      "Epoch: 100/100 | Batch 013/018 | Train/Val Loss: 0.04\n",
      "Epoch: 100/100 | Batch 014/018 | Train/Val Loss: 0.07\n",
      "Epoch: 100/100 | Batch 015/018 | Train/Val Loss: 0.02\n",
      "Epoch: 100/100 | Batch 016/018 | Train/Val Loss: 0.01\n",
      "Epoch: 100/100 | Batch 017/018 | Train/Val Loss: 0.04\n",
      "Epoch: 100/100 Train Acc: 0.993 Val Acc: 0.984\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "learning_rate = 0.05\n",
    "num_epochs = 100\n",
    "\n",
    "model = MultilayerPerceptron(num_feautures = 2, num_classes = 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "best_train_acc = 0.0\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "    \n",
    "    train_acc = compute_accuracy(model, train_loader)\n",
    "    val_acc = compute_accuracy(model, val_loader)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_train_acc = train_acc\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\", 'Train Acc: %.3f' % train_acc, 'Val Acc: %.3f' % val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.9844 Best train accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\", f\"Best train accuracy: {best_train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function takes as input a vector z of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](https://miro.medium.com/v2/resize:fit:1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0202, 0.9025, 0.0497, 0.0111, 0.0165])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.3, 5.1, 2.2, 0.7, 1.1])\n",
    "outputs = F.softmax(x, dim = 0)\n",
    "print(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actually happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0202, 0.9025, 0.0497, 0.0111, 0.0165])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.exp(x) / torch.exp(x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax accross rows: \n",
      " tensor([[0.2689, 0.9781, 0.7311, 0.0011, 0.0474],\n",
      "        [0.7311, 0.0219, 0.2689, 0.9989, 0.9526]])\n",
      "Softmax accross columns: \n",
      " tensor([[0.0202, 0.9025, 0.0497, 0.0111, 0.0165],\n",
      "        [0.0053, 0.0019, 0.0018, 0.9590, 0.0320]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.3, 5.1, 2.2, 0.7, 1.1],\n",
    "                  [2.3, 1.3, 1.2, 7.5, 4.1]])\n",
    "outputs = F.softmax(x, dim = 0)\n",
    "print('Softmax accross rows: \\n', outputs)\n",
    "outputs = F.softmax(x, dim = 1)\n",
    "print('Softmax accross columns: \\n', outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: \n",
      " tensor([[0.0202, 0.9025, 0.0497, 0.0111, 0.0165],\n",
      "        [0.0053, 0.0019, 0.0018, 0.9590, 0.0320]])\n",
      "Most likely class:  tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "probs = F.softmax(x, dim = 1)\n",
    "print('Probabilities: \\n', probs)\n",
    "\n",
    "# argmax returns the index of the maximum value\n",
    "# we need max index across columns\n",
    "most_likely_class = torch.argmax(probs, dim = 1)\n",
    "print('Most likely class: ', most_likely_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in our case we need to predict the class of the example. When we apply softmax we get probavilities. We need to choose the class with the highest probability. This is done by argmax function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does dim (dimension) mean here?\n",
    "The shape of our tensor x here is (2, 5) where 2 means 2 rows and 5 means 5 columns. When we specify dim=0, it means we want to apply softmax across rows. When we specify dim=1, it means we want to apply softmax across columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](https://androidkt.com/wp-content/uploads/2023/05/Selection_099.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Cross Entropy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-03-11-43-42.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases only one term will be non-zero. Because something can only belong to one class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's to see how Cross Etropy Loss works with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4786,  0.2170,  0.2445],\n",
      "        [-0.9200,  0.7587,  0.0682],\n",
      "        [ 0.1503, -1.6424,  0.7011],\n",
      "        [ 0.8171, -0.7246, -1.2948]])\n"
     ]
    }
   ],
   "source": [
    "# random output from a neural network for 4 training examples\n",
    "# We will have 4 classes\n",
    "outputs = torch.randn(4, 3)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3905, 0.3006, 0.3090],\n",
      "        [0.1106, 0.5924, 0.2970],\n",
      "        [0.3447, 0.0574, 0.5979],\n",
      "        [0.7490, 0.1603, 0.0906]])\n"
     ]
    }
   ],
   "source": [
    "activations = F.softmax(outputs, dim = 1) # dim = 1 is across columns\n",
    "print(activations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have activations which are actually predictions for 4 training examples and each has prediction of 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# suppose these are true labels\n",
    "y_labels = torch.tensor([2, 0, 1, 1]) # 4 training examples\n",
    "\n",
    "# Now we need to One Hot Encode these labels\n",
    "# 0 -> [1, 0, 0]\n",
    "# 1 -> [0, 1, 0]\n",
    "# 2 -> [0, 0, 1]\n",
    "# We can use PyTorch's built-in function\n",
    "y_labels_oh = F.one_hot(y_labels)\n",
    "print(y_labels_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of each training example:\n",
      " tensor([[0.0000, 0.0000, 1.1746],\n",
      "        [2.2022, 0.0000, 0.0000],\n",
      "        [0.0000, 2.8578, 0.0000],\n",
      "        [0.0000, 1.8306, 0.0000]])\n",
      "\n",
      "Overall Loss: tensor(8.0652)\n",
      "Avg Loss tensor(2.0163)\n"
     ]
    }
   ],
   "source": [
    "# Using the formula for cross entropy loss\n",
    "print('Loss of each training example:\\n', -1. * y_labels_oh * torch.log(activations))\n",
    "loss = -torch.sum(y_labels_oh * torch.log(activations))\n",
    "print('\\nOverall Loss:', loss)\n",
    "print('Avg Loss', loss / len(y_labels_oh))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really like how beautiful this is. We are using matrices and vectors to do all the calculations. This is the power of linear algebra. We can do all the calculations with just one line of code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we multiply y_labels_oh by torch.log(activations) we know that only one term will be non-zero. So we will get the log of the probability of the correct class. And when we multiply it by -1 we will get the negative log of the probability of the correct class. And this is exactly what we want."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's wrap this into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(nn_output, y_labels):\n",
    "    activations = F.softmax(nn_output, dim = 1)\n",
    "    y_labels_oh = F.one_hot(y_labels)\n",
    "    loss = -torch.sum(y_labels_oh * torch.log(activations))\n",
    "    avg_loss = loss / len(y_labels_oh)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output for 4 training examples:\n",
      " tensor([[ 0.8414,  0.3765,  1.1708],\n",
      "        [-0.4216, -1.7063, -1.3469],\n",
      "        [ 0.9570, -2.6469, -1.8086],\n",
      "        [-0.1210,  0.4247,  0.9643]])\n",
      "True labels: tensor([2, 0, 1, 1])\n",
      "Loss: tensor(1.5431)\n",
      "Correct loss using built-in function: tensor(1.5431)\n"
     ]
    }
   ],
   "source": [
    "# random output from a neural network for 4 training examples\n",
    "# We will have 4 classes\n",
    "outputs = torch.randn(4, 3)\n",
    "print('Sample output for 4 training examples:\\n', outputs)\n",
    "\n",
    "# suppose these are true labels\n",
    "y_labels = torch.tensor([2, 0, 1, 1]) # 4 training examples\n",
    "print('True labels:', y_labels)\n",
    "\n",
    "loss = cross_entropy(outputs, y_labels)\n",
    "print('Loss:', loss)\n",
    "\n",
    "print('Correct loss using built-in function:', F.cross_entropy(outputs, y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2*torch.log(torch.tensor(1/3.))/2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftmaxRegression Model using PyTorch\n",
    "It has one hidden layer consisting of 3 neurons (for 3 classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ss](softmax-regression-model.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sofmax regression is also a logistic regression, the only difference is that it can predict the probability of more than two classes. It uses softmax function as the hypothesis function. The softmax function is a generalization of the logistic function that \"squashes\" a K-dimensional vector z of arbitrary real values to a K-dimensional vector σ(z) of real values in the range [0, 1] that add up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, num_features) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.linear(x), dim = 0) # here we need dim = 0 because self.linear(x) will return a 1D tensor of length 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks\n",
    "![](multilayer-perceptron.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why do we need non-linear activation functions?\n",
    "\n",
    "Because if there was no activation function (or non-linearity) then the output would be just a simple linear function of the input. So it would be just a linear regression model. Linear models have limited power and they only can learn linear relationships between variables. So if we want to learn non-linear relationships we need to use non-linear activation functions.\n",
    "\n",
    "We can use Logistic activation or nowadays people mostly use very simple activation function called ReLU (Rectified Linear Unit). It is very simple and it is just max(0, x). It is very simple and it works very well in practice.\n",
    "\n",
    "---\n",
    "\n",
    "2) Wide vs Deep Neural Networks\n",
    "\n",
    "Wide neural networks have more number of neurons in a single layer. Deep neural networks have more number of layers. So the number of parameters in a wide neural network is more than the number of parameters in a deep neural network. So wide neural networks are more prone to overfitting. Deep neural networks are more prone to underfitting. Also, deep neural networks can suffer from gradient vanishing and gradient exploding problems. So we need to use some techniques to avoid these problems.\n",
    "\n",
    "---\n",
    "\n",
    "3) Why do we initialize weights randomly and not just zero?\n",
    "\n",
    "This is because if we initilized everything to zero. Then the hidden layer would have just one neuron. Because all the neurons would be computing the same thing. So we need to initialize weights randomly.\n",
    "\n",
    "![](random-weight-init.png)\n",
    "\n",
    "That is why we initialize weights to small random values. So that each neuron can have their own lives :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
